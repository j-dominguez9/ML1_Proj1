{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Adults Dataset</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 1 - Lab 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors:\n",
    "Richard Kim <br> Connor Dobbs<br> Joaquin Dominguez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hide deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full, clean dataset with original attributes\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/j-dominguez9/ML1_Proj1/main/Data/census_clean.csv')\n",
    "\n",
    "\n",
    "# OneHotEncoded dataset\n",
    "df_ohe = pd.read_csv('https://raw.githubusercontent.com/j-dominguez9/ML1_Proj1/main/Data/OHE_Adults.csv')\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we seperate the entire dataset into 'train' and 'test' with a 90/10 split. This 'test' set will be used as a hold out set for final evaluation after models have been hyperparameter tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40657\n"
     ]
    }
   ],
   "source": [
    "# create train, validation, and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df_2 = df_ohe.copy()\n",
    "\n",
    "np.random.seed(42)\n",
    "train, test = np.split(df_2.sample(frac=1), [int(.9*len(df_2))])\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import relevant function\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "### acquire target label values, delete column of said acquired values, and assign remaining variables and corresponding values to 'X'\n",
    "\n",
    "if 'income' in train:\n",
    "    y_train = train['income'].values \n",
    "    del train['income'] \n",
    "    X_train = train.values \n",
    "\n",
    "\n",
    "# target label values for test set\n",
    "if 'income' in test:\n",
    "    y_test = test['income'].values \n",
    "    del test['income'] \n",
    "    X_test = test.values \n",
    "\n",
    "#create train2 to keep version with age\n",
    "train2 = train.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redo above for age variable as outcome\n",
    "\n",
    "if 'age' in train:\n",
    "    yAge_train = train['age'].values \n",
    "    del train['age'] \n",
    "    XAge_train = train.values \n",
    "\n",
    "if 'age' in test:\n",
    "    yAge_test = test['age'].values \n",
    "    del test['age'] \n",
    "    XAge_test = test.values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize and Scale Data\n",
    "\n",
    "We will standardize the dataset to ensure that each variable is weighted equally, since there are many variables on very different scales as well as one hot encoded categorical variables. This should help reduce model bias, improving performance, as well as making feature importance easier to interpret later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#standard set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "X_train = scl_obj.transform(X_train) \n",
    "X_test = scl_obj.transform(X_test)\n",
    "\n",
    "#oversampled data \n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_over)\n",
    "X_over = scl_obj.transform(X_over) \n",
    "X_test_over = scl_obj.transform(X_test)\n",
    "\n",
    "#undersampled data\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_under)\n",
    "X_under = scl_obj.transform(X_under) \n",
    "X_test_under = scl_obj.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "\n",
    "In data preparation, we seperated the entire dataset into 'train' and 'test' with a 80/20 split. This 'test' set will be used for final evaluation after models have been hyperparameter tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the 'train' set, we create a 10-fold shuffle split cross validation object that can be used with the regular, over, and under sampled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import relevant function\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "### split data 80/20 with 10 cross-validation\n",
    "num_cv_iterations = 10\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2, random_state = 42)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific\n",
    "dataset and the stakeholders needs?\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of Data:\n",
    "\n",
    "This data was originally collected as part of the 1994 US Census. According to census.gov, The purpose of the Census is for the government to better understand the demographics of the people living there and where a variety of metrics are trending. This information is used to help determine where and how to allocate federal and state funding. The raw data was then extracted and compiled by Barry Becker and is available on UCI's machine learning repository here: https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "Accuracy: <br>\n",
    "> The first metric we will be using is Accuracy. By accuracy, we mean \"the ratio of number of correct predictions to the total number of input samples.\"[^1] \n",
    "\n",
    "> Accuracy = Number of Correct Predictions/Total Number of Predictions Made\n",
    "\n",
    "> By balancing our datset with respect to our target variable, we took measures to maximize the value that this metric provides. For our dataset, accuracy will give us a measure of how maany instances are correctly classified as >=50K  or <50K, in any given model.\n",
    "\n",
    "Precision: <br>\n",
    "> In order to get a more detailed look at what accuracy represents, we first break down the metric into Precision and Recall. Precision, in essence, outlines the proportion of positive identifications was actually correct[^2].\n",
    "\n",
    "> Precision = True Positive/(True Positive + False Positive)\n",
    "\n",
    "> For our dataset, Precision will give us a measure of the proportion of positives (>=50K) were classified correctly.\n",
    "\n",
    "Sensitivity/Recall: <br>\n",
    "\n",
    "> Recall, on the other hand, measures the proportion of actual positives that  were identified correctly[^3].\n",
    "\n",
    "> Recall = True Positive/(True Positive + False Negative)\n",
    "\n",
    "> For our dataset, Recall will give us a measure of how many of the instances that were actually >=50K were classified as such by the model.\n",
    "\n",
    "Specificity: <br>\n",
    "> Inversely to the previous measure, specificity (True Negative Rate) refers to the \"proportion of negative data points that are correctly considered as negative, with respect to all negative data points.\"[^5]\n",
    "\n",
    "> Sensitivity = True Negative/(True Negative + False Positive)\n",
    "\n",
    "> For our dataset, specificity measures the proportion of actual <50K classified as such by a given model.\n",
    "\n",
    "While we will look at the above metrics for income classifcation, the primary metric we will be looking to optimize in this case is Sensitivity, as it is the more useful metric in many contexts because it is harder to predict because it is a minority of people, and because predicting high earners could be used for things like targeted marketing campaigns for high margin products. \n",
    "\n",
    "[^1]: https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "[^2]: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "[^3]: https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall\n",
    "[^4]: https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "[^5]: https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234\n",
    "\n",
    "### Age Prediction Evaluation Metric\n",
    "RMSE: <br>\n",
    "> The main metric we will be using to evaluate our prediction models for age is RMSE, or root mean squared error. \n",
    "\n",
    "> The RMSE of a model will tell us how far our model predictions deviate from the actual values. For our dataset, RMSE will measure the standard deviation of our model's predicted age from the actual age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and adjust parameters.\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate and Compare\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Results\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the Ramifications\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling? How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\n",
    "\n",
    "*Delete before submitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have free reign to provide additional analyses or combine analyses.\n",
    "\n",
    "*Delete before submitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
